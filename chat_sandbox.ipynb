{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from main import init_env_vars\n",
    "from langchain import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "init_env_vars()\n",
    "\n",
    "db = FAISS.load_local('index', OpenAIEmbeddings())\n",
    "retriever = db.as_retriever(search_type='similarity', search_kwargs={'k': 4})\n",
    "\n",
    "\n",
    "def ask_bot(question, ai_instruction):\n",
    "    system_template = ai_instruction + \"\\n\\n{summaries}\"\n",
    "    messages = [\n",
    "        SystemMessagePromptTemplate.from_template(system_template),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\"),\n",
    "    ]\n",
    "    chain_type_kwargs = {\"prompt\": ChatPromptTemplate.from_messages(messages)}\n",
    "    qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "        llm=ChatOpenAI(temperature=0),\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs=chain_type_kwargs, verbose=True)\n",
    "    result = qa({\"question\": question})\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T12:06:54.577598544Z",
     "start_time": "2023-06-06T12:06:54.439950783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def get_sources(result):\n",
    "    source_urls = []\n",
    "    for document in result['source_documents']:\n",
    "        url = document.metadata['source'].replace(\"website/\", \"https://\")\n",
    "        if url not in source_urls:\n",
    "            source_urls.append(url)\n",
    "\n",
    "    return source_urls"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T12:06:55.529180714Z",
     "start_time": "2023-06-06T12:06:55.525047356Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new RetrievalQAWithSourcesChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "There are several retrieval chains that you can use for a question and answer bot, depending on your specific needs and preferences. Here are a few examples:\n",
      "\n",
      "1. ConversationalRetrievalChain: This chain condenses the current question and chat history into a standalone question, performs retrieval, and then answers the question using retrieval augmented generation with a separate model.\n",
      "\n",
      "2. RetrievalQA: This chain performs retrieval on a given index and then answers the question using a language model.\n",
      "\n",
      "3. GraphCypherQAChain: This chain performs retrieval on a graph database using Cypher queries and then answers the question using a language model.\n",
      "\n",
      "4. MultiRetrievalQAChain: This chain allows you to select from multiple retrieval models and then answers the question using a language model.\n",
      "\n",
      "5. FLARE: This chain uses a combination of retrieval and generation to answer questions, with a focus on explainability and interpretability.\n",
      "\n",
      "These are just a few examples, and there are many other retrieval chains available that you can use for your question and answer bot.\n",
      "['https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html', 'https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html', 'https://python.langchain.com/en/latest/modules/chains/how_to_guides.html']\n"
     ]
    }
   ],
   "source": [
    "# ai_instruction = \"Give your answer in a bullet point format to be used for slide presentations\"\n",
    "ai_instruction = \"\"\n",
    "question = \"What are the different retrieval chain I can use for question and answer bot?\"\n",
    "\n",
    "response = ask_bot(question, ai_instruction)\n",
    "\n",
    "print(response['answer'])\n",
    "print(get_sources(response))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T12:07:53.717990164Z",
     "start_time": "2023-06-06T12:07:27.633232316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "\n",
    "def ask_bot_conversational(question):\n",
    "    retriever = db.as_retriever(search_type='similarity', search_kwargs={'k': 4})\n",
    "    chat_history = []\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm=ChatOpenAI(temperature=0), retriever=retriever, verbose=True, return_source_documents=True)\n",
    "\n",
    "    return qa({\"question\": question, \"chat_history\": chat_history})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T12:08:49.889138708Z",
     "start_time": "2023-06-06T12:08:49.841245081Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "config\n",
      "\n",
      "dict\n",
      "\n",
      "*\n",
      "\n",
      "kwargs\n",
      "\n",
      "Any\n",
      "\n",
      ">\n",
      "\n",
      "StuffDocumentsChain\n",
      "\n",
      "if\n",
      "\n",
      "\"llm_chain\"\n",
      "\n",
      "in\n",
      "\n",
      "config\n",
      "\n",
      "llm_chain_config\n",
      "\n",
      "config\n",
      "\n",
      "pop\n",
      "\n",
      "\"llm_chain\"\n",
      "\n",
      "llm_chain\n",
      "\n",
      "load_chain_from_config\n",
      "\n",
      "llm_chain_config\n",
      "\n",
      "elif\n",
      "\n",
      "\"llm_chain_path\"\n",
      "\n",
      "in\n",
      "\n",
      "config\n",
      "\n",
      "llm_chain\n",
      "\n",
      "load_chain\n",
      "\n",
      "config\n",
      "\n",
      "pop\n",
      "\n",
      "\"llm_chain_path\"\n",
      "\n",
      "))\n",
      "\n",
      "else\n",
      "\n",
      "raise\n",
      "\n",
      "ValueError\n",
      "\n",
      "\"One of `llm_chain` or `llm_chain_config` must be present.\"\n",
      "\n",
      "if\n",
      "\n",
      "not\n",
      "\n",
      "isinstance\n",
      "\n",
      "llm_chain\n",
      "\n",
      "LLMChain\n",
      "\n",
      "):\n",
      "\n",
      "raise\n",
      "\n",
      "ValueError\n",
      "\n",
      "\"Expected LLMChain, got\n",
      "\n",
      "llm_chain\n",
      "\n",
      "if\n",
      "\n",
      "\"document_prompt\"\n",
      "\n",
      "in\n",
      "\n",
      "config\n",
      "\n",
      "prompt_config\n",
      "\n",
      "config\n",
      "\n",
      "pop\n",
      "\n",
      "\"document_prompt\"\n",
      "\n",
      "document_prompt\n",
      "\n",
      "load_prompt_from_config\n",
      "\n",
      "prompt_config\n",
      "\n",
      "elif\n",
      "\n",
      "\"document_prompt_path\"\n",
      "\n",
      "in\n",
      "\n",
      "config\n",
      "\n",
      "document_prompt\n",
      "\n",
      "load_prompt\n",
      "\n",
      "config\n",
      "\n",
      "pop\n",
      "\n",
      "\"document_prompt_path\"\n",
      "\n",
      "))\n",
      "\n",
      "else\n",
      "\n",
      "raise\n",
      "\n",
      "ValueError\n",
      "\n",
      "\"One of `document_prompt` or `document_prompt_path` must be present.\"\n",
      "\n",
      "return\n",
      "\n",
      "StuffDocumentsChain\n",
      "\n",
      "llm_chain\n",
      "\n",
      "llm_chain\n",
      "\n",
      "document_prompt\n",
      "\n",
      "document_prompt\n",
      "\n",
      "*\n",
      "\n",
      "config\n",
      "\n",
      "def\n",
      "\n",
      "_load_map_reduce_documents_chain\n",
      "\n",
      "config\n",
      "\n",
      "dict\n",
      "\n",
      "*\n",
      "\n",
      "kwargs\n",
      "\n",
      "Any\n",
      "\n",
      ">\n",
      "\n",
      "That’s it! For more details about how to do cool things with Chains, check out the how-to guide for chains.\n",
      "\n",
      "If you want more control and understanding over what is happening, please see the information below.\n",
      "\n",
      "The stuff Chain#\n",
      "\n",
      "This sections shows results of using the stuff Chain to do question answering.\n",
      "\n",
      "chain\n",
      "\n",
      "load_qa_chain\n",
      "\n",
      "OpenAI\n",
      "\n",
      "temperature\n",
      "\n",
      "),\n",
      "\n",
      "chain_type\n",
      "\n",
      "\"stuff\"\n",
      "\n",
      "query\n",
      "\n",
      "\"What did the president say about Justice Breyer\"\n",
      "\n",
      "chain\n",
      "\n",
      "({\n",
      "\n",
      "\"input_documents\"\n",
      "\n",
      "docs\n",
      "\n",
      "\"question\"\n",
      "\n",
      "query\n",
      "\n",
      "},\n",
      "\n",
      "return_only_outputs\n",
      "\n",
      "True\n",
      "\n",
      "{'output_text': ' The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.'}\n",
      "\n",
      "Custom Prompts\n",
      "\n",
      "You can also use your own prompts with this chain. In this example, we will respond in Italian.\n",
      "\n",
      "prompt_template\n",
      "\n",
      "\"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question:\n",
      "\n",
      "{question}\n",
      "\n",
      "Answer in Italian:\"\"\"\n",
      "\n",
      "PROMPT\n",
      "\n",
      "PromptTemplate\n",
      "\n",
      "template\n",
      "\n",
      "prompt_template\n",
      "\n",
      "input_variables\n",
      "\n",
      "\"context\"\n",
      "\n",
      "\"question\"\n",
      "\n",
      "page_content\n",
      "\n",
      "for\n",
      "\n",
      "in\n",
      "\n",
      "texts\n",
      "\n",
      "[:\n",
      "\n",
      "]]\n",
      "\n",
      "Quickstart#\n",
      "\n",
      "If you just want to get started as quickly as possible, this is the recommended way to do it:\n",
      "\n",
      "from\n",
      "\n",
      "langchain.chains.summarize\n",
      "\n",
      "import\n",
      "\n",
      "load_summarize_chain\n",
      "\n",
      "chain\n",
      "\n",
      "load_summarize_chain\n",
      "\n",
      "llm\n",
      "\n",
      "chain_type\n",
      "\n",
      "\"map_reduce\"\n",
      "\n",
      "chain\n",
      "\n",
      "run\n",
      "\n",
      "docs\n",
      "\n",
      "' In response to Russian aggression in Ukraine, the United States and its allies are taking action to hold Putin accountable, including economic sanctions, asset seizures, and military assistance. The US is also providing economic and humanitarian aid to Ukraine, and has passed the American Rescue Plan and the Bipartisan Infrastructure Law to help struggling families and create jobs. The US remains unified and determined to protect Ukraine and the free world.'\n",
      "\n",
      "If you want more control and understanding over what is happening, please see the information below.\n",
      "\n",
      "The stuff Chain#\n",
      "\n",
      "This sections shows results of using the stuff Chain to do summarization.\n",
      "\n",
      "chain\n",
      "\n",
      "load_summarize_chain\n",
      "\n",
      "llm\n",
      "\n",
      "chain_type\n",
      "Human: What is StuffDocumentsChain chain?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "StuffDocumentsChain is a chain that can be used for question answering and summarization tasks. It takes in a list of documents and a query or summary prompt, and returns the answer or summary based on the input. It can also be customized with user-defined prompts.\n",
      "['https://python.langchain.com/en/latest/_modules/langchain/chains/loading.html', 'https://python.langchain.com/en/latest/modules/chains/getting_started.html', 'https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html', 'https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html']\n"
     ]
    }
   ],
   "source": [
    "question = \"What is StuffDocumentsChain chain?\"\n",
    "result = ask_bot_conversational(question)\n",
    "\n",
    "print(result['answer'])\n",
    "print(get_sources(result))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T12:09:14.321181518Z",
     "start_time": "2023-06-06T12:09:08.134282208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new StuffDocumentsChain chain...\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Chat Prompt Templates#\n",
      "\n",
      "Chat Models take a list of chat messages as input - this list commonly referred to as a prompt.\n",
      "These chat messages differ from raw string (which you would pass into a LLM model) in that every message is associated with a role.\n",
      "\n",
      "For example, in OpenAI Chat Completion API, a chat message can be associated with the AI, human or system role. The model is supposed to follow instruction from system chat message more closely.\n",
      "\n",
      "LangChain provides several prompt templates to make constructing and working with prompts easily. You are encouraged to use these chat related prompt templates instead of PromptTemplate when querying chat models to fully exploit the potential of underlying chat model.\n",
      "\n",
      "from\n",
      "\n",
      "langchain.prompts\n",
      "\n",
      "import\n",
      "\n",
      "ChatPromptTemplate\n",
      "\n",
      "PromptTemplate\n",
      "\n",
      "SystemMessagePromptTemplate\n",
      "\n",
      "AIMessagePromptTemplate\n",
      "\n",
      "HumanMessagePromptTemplate\n",
      "\n",
      "from\n",
      "\n",
      "langchain.schema\n",
      "\n",
      "import\n",
      "\n",
      "AIMessage\n",
      "\n",
      "HumanMessage\n",
      "\n",
      "SystemMessage\n",
      "\n",
      "Different types of MessagePromptTemplate#\n",
      "\n",
      "LangChain provides different types of MessagePromptTemplate. The most commonly used are AIMessagePromptTemplate, SystemMessagePromptTemplate and HumanMessagePromptTemplate, which create an AI message, system message and human message respectively.\n",
      "\n",
      "However, in cases where the chat model supports taking chat message with arbitrary role, you can use ChatMessagePromptTemplate, which allows user to specify the role name.\n",
      "\n",
      "from\n",
      "\n",
      "langchain.prompts\n",
      "\n",
      "import\n",
      "\n",
      "ChatMessagePromptTemplate\n",
      "\n",
      "prompt\n",
      "\n",
      "\"May the\n",
      "\n",
      "{subject}\n",
      "\n",
      "be with you\"\n",
      "\n",
      "chat_message_prompt\n",
      "\n",
      "ChatMessagePromptTemplate\n",
      "\n",
      "from_template\n",
      "\n",
      "role\n",
      "\n",
      "\"Jedi\"\n",
      "\n",
      "template\n",
      "\n",
      "prompt\n",
      "\n",
      "chat_message_prompt\n",
      "\n",
      "format\n",
      "\n",
      "subject\n",
      "\n",
      "\"force\"\n",
      "\n",
      "ChatMessage(content='May the force be with you', additional_kwargs={}, role='Jedi')\n",
      "\n",
      "SystemMessagePromptTemplate\n",
      "\n",
      "HumanMessagePromptTemplate\n",
      "\n",
      "chat\n",
      "\n",
      "ChatOpenAI\n",
      "\n",
      "temperature\n",
      "\n",
      "template\n",
      "\n",
      "\"You are a helpful assistant that translates\n",
      "\n",
      "{input_language}\n",
      "\n",
      "to\n",
      "\n",
      "{output_language}\n",
      "\n",
      ".\"\n",
      "\n",
      "system_message_prompt\n",
      "\n",
      "SystemMessagePromptTemplate\n",
      "\n",
      "from_template\n",
      "\n",
      "template\n",
      "\n",
      "human_template\n",
      "\n",
      "{text}\n",
      "\n",
      "human_message_prompt\n",
      "\n",
      "HumanMessagePromptTemplate\n",
      "\n",
      "from_template\n",
      "\n",
      "human_template\n",
      "\n",
      "chat_prompt\n",
      "\n",
      "ChatPromptTemplate\n",
      "\n",
      "from_messages\n",
      "\n",
      "([\n",
      "\n",
      "system_message_prompt\n",
      "\n",
      "human_message_prompt\n",
      "\n",
      "])\n",
      "\n",
      "chain\n",
      "\n",
      "LLMChain\n",
      "\n",
      "llm\n",
      "\n",
      "chat\n",
      "\n",
      "prompt\n",
      "\n",
      "chat_prompt\n",
      "\n",
      "chain\n",
      "\n",
      "run\n",
      "\n",
      "input_language\n",
      "\n",
      "\"English\"\n",
      "\n",
      "output_language\n",
      "\n",
      "\"French\"\n",
      "\n",
      "text\n",
      "\n",
      "\"I love programming.\"\n",
      "\n",
      "# -> \"J'aime programmer.\"\n",
      "\n",
      "Agents with Chat Models#\n",
      "\n",
      "Agents can also be used with chat models, you can initialize one using AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION as the agent type.\n",
      "\n",
      "from\n",
      "\n",
      "langchain.agents\n",
      "\n",
      "import\n",
      "\n",
      "load_tools\n",
      "\n",
      "from\n",
      "\n",
      "langchain.agents\n",
      "\n",
      "import\n",
      "\n",
      "initialize_agent\n",
      "\n",
      "from\n",
      "\n",
      "langchain.agents\n",
      "\n",
      "import\n",
      "\n",
      "AgentType\n",
      "\n",
      "from\n",
      "\n",
      "langchain.chat_models\n",
      "\n",
      "import\n",
      "\n",
      "ChatOpenAI\n",
      "\n",
      "from\n",
      "\n",
      "How to work with partial Prompt Templates#\n",
      "\n",
      "A prompt template is a class with a .format method which takes in a key-value map and returns a string (a prompt) to pass to the language model. Like other methods, it can make sense to “partial” a prompt template - eg pass in a subset of the required values, as to create a new prompt template which expects only the remaining subset of values.\n",
      "\n",
      "LangChain supports this in two ways: we allow for partially formatted prompts (1) with string values, (2) with functions that return string values. These two different ways support different use cases. In the documentation below we go over the motivations for both use cases as well as how to do it in LangChain.\n",
      "\n",
      "Partial With Strings#\n",
      "Human: How to use ChatPromptTemplate?\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "ChatPromptTemplate is a class provided by LangChain that allows you to create chat prompts easily. Here's an example of how to use it:\n",
      "\n",
      "```\n",
      "from langchain.prompts import ChatPromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
      "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
      "\n",
      "# Create a chat prompt template\n",
      "chat_prompt_template = ChatPromptTemplate([\n",
      "    SystemMessagePromptTemplate(\"Welcome to our chatbot! How can I assist you today?\"),\n",
      "    HumanMessagePromptTemplate(),\n",
      "    AIMessagePromptTemplate()\n",
      "])\n",
      "\n",
      "# Create a chat prompt from the template\n",
      "chat_prompt = chat_prompt_template.format({\n",
      "    \"human_message\": HumanMessage(content=\"I need help with my order\"),\n",
      "    \"ai_message\": AIMessage(content=\"Sure, I'd be happy to help! What's your order number?\")\n",
      "})\n",
      "\n",
      "# Pass the chat prompt to a chat model\n",
      "chat_model = ChatOpenAI()\n",
      "response = chat_model.generate_response(chat_prompt)\n",
      "```\n",
      "\n",
      "In this example, we create a chat prompt template that consists of a system message, a human message, and an AI message. We then create a chat prompt from the template by passing in a human message and an AI message. Finally, we pass the chat prompt to a chat model (in this case, ChatOpenAI) to generate a response.\n",
      "['https://python.langchain.com/en/latest/modules/prompts/chat_prompt_template.html', 'https://python.langchain.com/en/latest/getting_started/getting_started.html', 'https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/partial.html']\n"
     ]
    }
   ],
   "source": [
    "question = \"How to use ChatPromptTemplate?\"\n",
    "result = ask_bot_conversational(question)\n",
    "\n",
    "print(result['answer'])\n",
    "print(get_sources(result))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T12:11:31.805657344Z",
     "start_time": "2023-06-06T12:11:07.851049646Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
